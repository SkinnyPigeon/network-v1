{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This might be a good bet since there is already known noise in the system and the data is tabular so perhaps will work right out of the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Dict\n",
    "from statistics import mean, stdev\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, patches\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import preprocessing\n",
    "from IPython.display import HTML, Image\n",
    "from scipy.stats import norm\n",
    "\n",
    "matplotlib.rcParams[\"animation.embed_limit\"] = 2**128\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    df = df[[\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Embarked\", \"PassengerId\"]]\n",
    "    df[\"Age\"].fillna((df[\"Age\"].median()))\n",
    "    oh = pd.get_dummies(df[\"Sex\"])\n",
    "    df = df.drop(\"Sex\", axis=1)\n",
    "    df = df.join(oh)\n",
    "    df[\"Embarked\"] = df[\"Embarked\"].replace([\"S\"], 1)\n",
    "    df[\"Embarked\"] = df[\"Embarked\"].replace([\"C\"], 2)\n",
    "    df[\"Embarked\"] = df[\"Embarked\"].replace([\"Q\"], 3)\n",
    "    df[\"Embarked\"] = df[\"Embarked\"].fillna(4)\n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/titanic/train.csv\")\n",
    "df = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NODES = 10\n",
    "COLUMNS = [\"Pclass\", \"male\", \"female\", \"Age\", \"SibSp\", \"Parch\", \"Embarked\", \"PassengerId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 10)                90        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101\n",
      "Trainable params: 101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units=NODES, input_shape=[8]))\n",
    "model.add(tf.keras.layers.Dense(units=1))\n",
    "model.summary()\n",
    "opt = Adam(0.01)\n",
    "model.compile(optimizer=opt, loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_history(columns: int, nodes: int) -> Dict:\n",
    "    history = {i:{j: [] for j in range(nodes)} for i in range(columns)}\n",
    "    history[\"loss\"] = []\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = create_history(8, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df[COLUMNS]\n",
    "y_train = df[[\"Survived\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING EPOCH 1\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4083\n",
      "RUNNING EPOCH 2\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4046\n",
      "RUNNING EPOCH 3\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4038\n",
      "RUNNING EPOCH 4\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4054\n",
      "RUNNING EPOCH 5\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4037\n",
      "RUNNING EPOCH 6\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4026\n",
      "RUNNING EPOCH 7\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4030\n",
      "RUNNING EPOCH 8\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4038\n",
      "RUNNING EPOCH 9\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4020\n",
      "RUNNING EPOCH 10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4020\n",
      "RUNNING EPOCH 11\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4019\n",
      "RUNNING EPOCH 12\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4019\n",
      "RUNNING EPOCH 13\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4018\n",
      "RUNNING EPOCH 14\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4021\n",
      "RUNNING EPOCH 15\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4022\n",
      "RUNNING EPOCH 16\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4027\n",
      "RUNNING EPOCH 17\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4018\n",
      "RUNNING EPOCH 18\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4015\n",
      "RUNNING EPOCH 19\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4016\n",
      "RUNNING EPOCH 20\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4016\n",
      "RUNNING EPOCH 21\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4006\n",
      "RUNNING EPOCH 22\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4007\n",
      "RUNNING EPOCH 23\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4028\n",
      "RUNNING EPOCH 24\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4007\n",
      "RUNNING EPOCH 25\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4007\n",
      "RUNNING EPOCH 26\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4006\n",
      "RUNNING EPOCH 27\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4007\n",
      "RUNNING EPOCH 28\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4004\n",
      "RUNNING EPOCH 29\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4014\n",
      "RUNNING EPOCH 30\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4005\n",
      "RUNNING EPOCH 31\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4005\n",
      "RUNNING EPOCH 32\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4005\n",
      "RUNNING EPOCH 33\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4007\n",
      "RUNNING EPOCH 34\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4013\n",
      "RUNNING EPOCH 35\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4003\n",
      "RUNNING EPOCH 36\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4000\n",
      "RUNNING EPOCH 37\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4004\n",
      "RUNNING EPOCH 38\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4009\n",
      "RUNNING EPOCH 39\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4009\n",
      "RUNNING EPOCH 40\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4040\n",
      "RUNNING EPOCH 41\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4022\n",
      "RUNNING EPOCH 42\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4025\n",
      "RUNNING EPOCH 43\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4048\n",
      "RUNNING EPOCH 44\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4019\n",
      "RUNNING EPOCH 45\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4006\n",
      "RUNNING EPOCH 46\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4003\n",
      "RUNNING EPOCH 47\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4003\n",
      "RUNNING EPOCH 48\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.4016\n",
      "RUNNING EPOCH 49\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4023\n",
      "RUNNING EPOCH 50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4028\n",
      "RUNNING EPOCH 51\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3982\n",
      "RUNNING EPOCH 52\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.4074\n",
      "RUNNING EPOCH 53\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4046\n",
      "RUNNING EPOCH 54\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4005\n",
      "RUNNING EPOCH 55\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4005\n",
      "RUNNING EPOCH 56\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4005\n",
      "RUNNING EPOCH 57\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.4005\n",
      "RUNNING EPOCH 58\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.4005\n",
      "RUNNING EPOCH 59\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4005\n",
      "RUNNING EPOCH 60\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4005\n",
      "RUNNING EPOCH 61\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.4004\n",
      "RUNNING EPOCH 62\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.4004\n",
      "RUNNING EPOCH 63\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4015\n",
      "RUNNING EPOCH 64\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4007\n",
      "RUNNING EPOCH 65\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3993\n",
      "RUNNING EPOCH 66\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3977\n",
      "RUNNING EPOCH 67\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3980\n",
      "RUNNING EPOCH 68\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3990\n",
      "RUNNING EPOCH 69\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3978\n",
      "RUNNING EPOCH 70\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3982\n",
      "RUNNING EPOCH 71\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3968\n",
      "RUNNING EPOCH 72\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3992\n",
      "RUNNING EPOCH 73\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3982\n",
      "RUNNING EPOCH 74\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3988\n",
      "RUNNING EPOCH 75\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3981\n",
      "RUNNING EPOCH 76\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3977\n",
      "RUNNING EPOCH 77\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3962\n",
      "RUNNING EPOCH 78\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3949\n",
      "RUNNING EPOCH 79\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3962\n",
      "RUNNING EPOCH 80\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3972\n",
      "RUNNING EPOCH 81\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4001\n",
      "RUNNING EPOCH 82\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3981\n",
      "RUNNING EPOCH 83\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3977\n",
      "RUNNING EPOCH 84\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3999\n",
      "RUNNING EPOCH 85\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4006\n",
      "RUNNING EPOCH 86\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3963\n",
      "RUNNING EPOCH 87\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3947\n",
      "RUNNING EPOCH 88\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3984\n",
      "RUNNING EPOCH 89\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3963\n",
      "RUNNING EPOCH 90\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3952\n",
      "RUNNING EPOCH 91\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3956\n",
      "RUNNING EPOCH 92\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3937\n",
      "RUNNING EPOCH 93\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3936\n",
      "RUNNING EPOCH 94\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3940\n",
      "RUNNING EPOCH 95\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3973\n",
      "RUNNING EPOCH 96\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3953\n",
      "RUNNING EPOCH 97\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3979\n",
      "RUNNING EPOCH 98\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3977\n",
      "RUNNING EPOCH 99\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3975\n",
      "RUNNING EPOCH 100\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3952\n",
      "RUNNING EPOCH 101\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3931\n",
      "RUNNING EPOCH 102\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3842\n",
      "RUNNING EPOCH 103\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3957\n",
      "RUNNING EPOCH 104\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3951\n",
      "RUNNING EPOCH 105\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3943\n",
      "RUNNING EPOCH 106\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3966\n",
      "RUNNING EPOCH 107\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3949\n",
      "RUNNING EPOCH 108\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3949\n",
      "RUNNING EPOCH 109\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3949\n",
      "RUNNING EPOCH 110\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3949\n",
      "RUNNING EPOCH 111\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3946\n",
      "RUNNING EPOCH 112\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3941\n",
      "RUNNING EPOCH 113\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3951\n",
      "RUNNING EPOCH 114\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3949\n",
      "RUNNING EPOCH 115\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3948\n",
      "RUNNING EPOCH 116\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3937\n",
      "RUNNING EPOCH 117\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3936\n",
      "RUNNING EPOCH 118\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3935\n",
      "RUNNING EPOCH 119\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3935\n",
      "RUNNING EPOCH 120\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3935\n",
      "RUNNING EPOCH 121\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3935\n",
      "RUNNING EPOCH 122\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3935\n",
      "RUNNING EPOCH 123\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3934\n",
      "RUNNING EPOCH 124\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3934\n",
      "RUNNING EPOCH 125\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3927\n",
      "RUNNING EPOCH 126\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3957\n",
      "RUNNING EPOCH 127\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3937\n",
      "RUNNING EPOCH 128\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3953\n",
      "RUNNING EPOCH 129\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3927\n",
      "RUNNING EPOCH 130\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3935\n",
      "RUNNING EPOCH 131\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3935\n",
      "RUNNING EPOCH 132\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3943\n",
      "RUNNING EPOCH 133\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3927\n",
      "RUNNING EPOCH 134\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3931\n",
      "RUNNING EPOCH 135\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3926\n",
      "RUNNING EPOCH 136\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3923\n",
      "RUNNING EPOCH 137\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3925\n",
      "RUNNING EPOCH 138\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3923\n",
      "RUNNING EPOCH 139\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3927\n",
      "RUNNING EPOCH 140\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3926\n",
      "RUNNING EPOCH 141\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3926\n",
      "RUNNING EPOCH 142\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3932\n",
      "RUNNING EPOCH 143\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3926\n",
      "RUNNING EPOCH 144\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3939\n",
      "RUNNING EPOCH 145\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3937\n",
      "RUNNING EPOCH 146\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.5050\n",
      "RUNNING EPOCH 147\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.5882\n",
      "RUNNING EPOCH 148\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.5882\n",
      "RUNNING EPOCH 149\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.5882\n",
      "RUNNING EPOCH 150\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.5882\n",
      "RUNNING EPOCH 151\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.5882\n",
      "RUNNING EPOCH 152\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.5882\n",
      "RUNNING EPOCH 153\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.5882\n",
      "RUNNING EPOCH 154\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.5881\n",
      "RUNNING EPOCH 155\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.5877\n",
      "RUNNING EPOCH 156\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.5874\n",
      "RUNNING EPOCH 157\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.5856\n",
      "RUNNING EPOCH 158\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.5462\n",
      "RUNNING EPOCH 159\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4698\n",
      "RUNNING EPOCH 160\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 161\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 162\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 163\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 164\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 165\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 166\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 167\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 168\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 169\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 170\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 171\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 172\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 173\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 174\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 175\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 176\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 177\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 178\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 179\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 180\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 181\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 182\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 183\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 184\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 185\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 186\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 187\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 188\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 189\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 190\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 191\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 192\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 193\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 194\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 195\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 196\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 197\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 198\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 199\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 200\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 201\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 202\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 203\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 204\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 205\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 206\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 207\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 208\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 209\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 210\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 211\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 212\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 213\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 214\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 215\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 216\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 217\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 218\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 219\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 220\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 221\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 222\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 223\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 224\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 225\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 226\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 227\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 228\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 229\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 230\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 231\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 232\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 233\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 234\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 235\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 236\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 237\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 238\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 239\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 240\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 241\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 242\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 243\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 244\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 245\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 246\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 247\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 248\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 249\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 250\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 251\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 252\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 253\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4062\n",
      "RUNNING EPOCH 254\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.4047"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/e.blackledge/Development/play/network/known_datasets/titanic.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/e.blackledge/Development/play/network/known_datasets/titanic.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m501\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/e.blackledge/Development/play/network/known_datasets/titanic.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRUNNING EPOCH \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/e.blackledge/Development/play/network/known_datasets/titanic.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     hist \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/e.blackledge/Development/play/network/known_datasets/titanic.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     history[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(hist\u001b[39m.\u001b[39mhistory[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/e.blackledge/Development/play/network/known_datasets/titanic.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     all_weights \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_weights()[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/miniforge3/envs/network/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/network/lib/python3.8/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniforge3/envs/network/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/network/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniforge3/envs/network/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniforge3/envs/network/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/network/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniforge3/envs/network/lib/python3.8/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniforge3/envs/network/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 501):\n",
    "    print(f\"RUNNING EPOCH {epoch}\")\n",
    "    hist = model.fit(x_train, y_train, epochs=1)\n",
    "    history[\"loss\"].append(hist.history[\"loss\"])\n",
    "    all_weights = model.layers[0].get_weights()[0].tolist()\n",
    "    for column in range(len(all_weights)):\n",
    "        for node in range(len(all_weights[column])):\n",
    "            history[column][node].append(all_weights[column][node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 3ms/step\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/titanic/eval.csv\")\n",
    "df = preprocess_data(df)\n",
    "x_test = df[COLUMNS]\n",
    "y_test = df[[\"Survived\"]]\n",
    "res = model.predict(x_test[COLUMNS])\n",
    "print(res)\n",
    "# df[\"Prediction\"] = model.predict(x_test[COLUMNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in res:\n",
    "    if i[0] == 1:\n",
    "        print(\"YEP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(x_test[COLUMNS])\n",
    "res = np.array([sigmoid(abs(i)) for i in res])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 0\n",
    "\n",
    "def get_groups(index: int, nodes: int) -> List:\n",
    "    group = []\n",
    "    for i in range(nodes):\n",
    "        group.append(history[index][i][start_index:])\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs = get_groups(0, NODES)\n",
    "ms = get_groups(1, NODES)\n",
    "fs = get_groups(2, NODES)\n",
    "aa = get_groups(3, NODES)\n",
    "ss = get_groups(4, NODES)\n",
    "ps = get_groups(5, NODES)\n",
    "es = get_groups(6, NODES)\n",
    "pas = get_groups(7, NODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light = \"#90CCF4\"\n",
    "darker = \"#5DA2D5\"\n",
    "important = \"#F3D250\"\n",
    "noise = \"#F78888\"\n",
    "other = \"#ECECEC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_box_plot(ax, data: List, color: str, label: str, position_offset: int = 0, showfliers: bool = False):\n",
    "    ax.set_xlabel(\"Input Column and Node Number\")\n",
    "    ax.set_ylabel(\"Weight Values\")\n",
    "    color = {\"color\": color}\n",
    "    ax.boxplot(\n",
    "        data,\n",
    "        positions=[i + position_offset for i in range(1,5)],\n",
    "        boxprops=color,\n",
    "        medianprops=color,\n",
    "        whiskerprops=color,\n",
    "        capprops=color,\n",
    "        flierprops={\"markeredgecolor\": other},\n",
    "        showfliers=showfliers,\n",
    "        labels=[f\"{label}{i}\" for i in range(1,5)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,6))\n",
    "generate_box_plot(ax, pcs, darker, \"Class\", showfliers=True)\n",
    "generate_box_plot(ax, ms, important, \"Male\", 4, showfliers=True)\n",
    "generate_box_plot(ax, fs, light, \"Female\", 8, showfliers=True)\n",
    "generate_box_plot(ax, aa, noise, \"Age\", 12, showfliers=True)\n",
    "generate_box_plot(ax, ss, darker, \"SibSpo\", 16, showfliers=True)\n",
    "generate_box_plot(ax, ps, important, \"ParChi\", 20, showfliers=True)\n",
    "generate_box_plot(ax, es, light, \"Embarked\", 24, showfliers=True)\n",
    "generate_box_plot(ax, pas, noise, \"ID\", 28, showfliers=True)\n",
    "for label in ax.get_xticklabels():\n",
    "    label.set_rotation(90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numbers(index: int) -> List:\n",
    "    one = history[0][index][start_index:]\n",
    "    two = history[1][index][start_index:]\n",
    "    three = history[2][index][start_index:]\n",
    "    four = history[3][index][start_index:]\n",
    "    five = history[4][index][start_index:]\n",
    "    six = history[5][index][start_index:]\n",
    "    seven = history[6][index][start_index:]\n",
    "    eight = history[7][index][start_index:]\n",
    "    return [one, two, three, four, five, six, seven, eight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_one = get_numbers(0)\n",
    "group_two = get_numbers(1)\n",
    "group_three = get_numbers(2)\n",
    "group_four = get_numbers(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('network')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4675c3144d5408229f03762137a1293c1ae58345d8260259851756cf0dcf4741"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
